from bs4 import BeautifulSoup
import requests
from datetime import datetime
import json
import time
import os
import urllib.request
import io
import gzip
import config
import numpy as np
import warnings
from tqdm import tqdm



def get_all_links():
    """
    Gets all the gz files url links from the sitemap of ApkPure
    Return: A list with all the gz url links
    """
    url = ('https://apkpure.com/sitemap.xml') # URL to the sitemap
    response = requests.get(url)    
    urlText = response.text #Grabs the content of the request object
    soup = BeautifulSoup(urlText, 'lxml') # Creates the HTML to soup object
    gz_list = []   # List to store all gz file url's
    for link in soup.find_all('a', href=True):
        gz_list.append(link['href']) #Gets the href tag
    gz_list = [i.text for i in soup.find_all('loc')]  #Grabs the link from the href tag
    return gz_list


def get_xml(url):
    """
    Takes the url for the gz file and writes it as an xml file 
    url: The Url file name 
    Returns: The name of the downloaded linked file
    """
    response = requests.get(url)
    compressed_file = io.BytesIO(response.content)
    decompressed_file = gzip.GzipFile(fileobj=compressed_file)
    OUTFILE_PATH = '.'.join(url.split('/')[-1].split('.')[:-1])
    with open(OUTFILE_PATH, 'wb') as outfile:
        outfile.write(decompressed_file.read())
    return OUTFILE_PATH

def get_links_from_xml(xml):
    """
    Extracts all the links from the XML file
    xml: Name of xml file 
    Returns: A list with all Url's from the XML file
    """
    file = open(xml, 'r') #Creates a handle to open xml file
    cont= file.read()  #reads the content of the file 
    command = 'rm ' + xml
    os.system(command)
    soup = BeautifulSoup(cont, 'lxml') #creates a soup object of the xml file
    links = soup.find_all('loc') # grabs all the links in the xml file 
    return [link.text for link in links]



def create_apk(url, outpath):
    response = requests.get(url) #creates a response object with the home page of the app url
    urlText = response.text  # grabs the text of the page 
    soup = BeautifulSoup(urlText, features="html") #creates the urltext into a soup object 
    if soup.find('div', attrs ={'class','ny-down'}) != None: #if this div class exists in the page 
        down_link = soup.find('div', attrs ={'class','ny-down'}).find('a', attrs={'class':'da'}).get('href') 
    else: 
        down_link = soup.find('div', attrs ={'class','down'}).find('a').get('href') #grabs the link to the download page
        
    url = 'https://apkpure.com'+down_link  # add link extension 
    response = requests.get(url)
    urlText = response.text
    soup = BeautifulSoup(urlText,  features="html")
    if not os.path.isdir('./'+outpath ): 
        os.mkdir(outpath) #creates directory if it doesnt exist
    if not os.path.isdir('./'+outpath + '/apk_files'):
        direc = outpath + '/apk_files'
        os.mkdir(direc)
    if soup.find('a', attrs = {'id':'download_link'}) == None:
        return False #returns no link if webpage requires verifiction 
    a =soup.find('a', attrs = {'id':'download_link'}).get('href') #finds the link to download 
    resp = requests.get(a, stream = True) 
    data = resp.content #gets content of apk files 
    app_name = url.split('/')[3]  #get names of apk files 
    folder = os.path.abspath('')  
    out  = os.path.join(folder, outpath+'/'+ 'apk_files/' + app_name+'.apk') #output file location 
    if len(out) > 200:
        return False
    
    
    with open(out, 'wb') as fh:
        fh.write(data)
    return app_name
    
def get_smalli(outpath, app_name):
    if not os.path.isdir('./'+outpath + '/smali'):
        direc = outpath + '/smali'
        os.mkdir(direc)
    command = 'apktool d ' + outpath+ '/apk_files/' + app_name + '.apk' #+ ' -o ' +  outpath + '/smali' 
    os.system(command)
  
    command3 = 'mv ' +app_name + ' '+outpath+'/smali/' #moves file to output folder
    os.system(command3)



